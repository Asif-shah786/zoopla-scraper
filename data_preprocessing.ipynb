{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ijBsPrWwl_Wq"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Real Estate Data Preprocessing Pipeline\n",
        "Notebook for cleaning and preparing property data for RAG pipeline\n"
      ],
      "metadata": {
        "id": "ijBsPrWwl_Wq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "from typing import Optional, List\n",
        "\n",
        "print(\"‚úì Libraries imported successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-ZS_CAflUUb",
        "outputId": "0b4f1719-e3c3-4721-ecc7-b6f342b3e564"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Libraries imported successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71f4360b"
      },
      "source": [],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34f1d3c9",
        "outputId": "22e56f38-047a-40bc-9ef4-b109f885348b"
      },
      "source": [
        "JSON_FILE_PATH = \"/content/19-august-12am_page1-ALL_property1-908.json\"\n",
        "OUTPUT_DIR = \"ready_data\"\n",
        "\n",
        "try:\n",
        "    df = pd.read_json(JSON_FILE_PATH)\n",
        "    print(\"‚úì JSON file loaded successfully!\")\n",
        "    print(f\"   Original dataset shape: {df.shape}\")\n",
        "    print(f\"   Columns found: {list(df.columns)}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"‚ùå File not found: {JSON_FILE_PATH}\")\n",
        "    print(\"Please upload your JSON file to Colab and update the path above\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading file: {e}\")"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì JSON file loaded successfully!\n",
            "   Original dataset shape: (908, 134)\n",
            "   Columns found: ['page', 'scraped_at', 'source', 'property_id', 'property_url', 'listing_id', 'price', 'property_type', 'tenure', 'bedrooms', 'bathrooms', 'receptions', 'has_epc', 'has_floorplan', 'display_address', 'outcode', 'agent', 'chain_free', 'title', 'status', 'number_of_photos', 'number_of_floorplans', 'address', 'council_tax_band', 'ground_rent', 'room__m', 'about_property', 'nearest_schools', 'nearest_stations', 'latitude', 'longitude', 'nearest_stations_distances', 'epc_rating', 'size_sq_feet', 'price_per_sqft', 'room_lounge_m', 'room_dining_room_m', 'room_office_m', 'room_kitchen_m', 'room_bedroom_one_m', 'room_bedroom_two_m', 'room_bedroom_three_m', 'room_bedroom_four_m', 'room_family_bathroom_m', 'room_master_bedroom_m', 'room_shower_room_m', 'room_bathroom_m', 'room_en_suite_m', 'room_porch_m', 'room_entrance_hallway_m', 'room_diner_m', 'room_reception_room_one_m', 'room_reception_room_two_m', 'room_kitchen_diner_m', 'room_utility_room_m', 'room_hall_m', 'room_dining_m', 'room_landing_m', 'room_dining_kitchen_m', 'room_bedroom_m', 'room_suite_m', 'room_entrance_vestibule_m', 'room_living_room_m', 'room_entrance_porch_m', 'room_orangery_m', 'room_under_stairs_storage_m', 'room_first_floor_landing_m', 'room_primary_bedroom_m', 'room_secondary_bedroom_m', 'room_dining_area_m', 'room_wet_room_m', 'room_conservatory_m', 'room_entry_m', 'room_sunroom_m', 'room_hallway_m', 'room_study_m', 'room_wc_m', 'room_c_m', 'room_ensuite_m', 'room_cellar_m', 'room_dressing_room_m', 'room_entrance_hall_m', 'room_family_room_m', 'room_utility_cupboard_m', 'room_workshop_m', 'room_utility_m', 'room_sun_room_m', 'room_bedroom_five_m', 'room_loft_room_m', 'room_garage_m', 'room_understairs_storage_m', 'room_downstairs_shower_room_m', 'room_basement_m', 'room_max_to_alcove_m', 'room_max_into_shower_recess_itself_m', 'room_to_the_cove_m', 'room_sitting_area_m', 'room_second_bedroom_m', 'room_third_bedroom_m', 'room_downstairs_wc_m', 'room_morning_room_m', 'room_open_plan_lounge_and_kitchen_m', 'room_second_reception_room_m', 'room_office_room_m', 'room_double_garage_m', 'room_sitting_room_m', 'room_laundry_m', 'room_ground_floor_m', 'room_floor_one_m', 'room_storage_m', 'room_landing_two_m', 'room_master_bedroom_and_en_suite_m', 'room_entrance_m', 'room_reception_room_m', 'room_snug_m', 'room_lounge_diner_m', 'room_kitchen_and_dining_room_m', 'room_reception_one_m', 'room_reception_two_m', 'room_reception_three_m', 'room_guest_cloakroom_m', 'room_fourth_bedroom_m', 'room_stairway_m', 'room_integral_garage_m', 'room_second_floor_landing_m', 'room_reception_room_three_m', 'room_play_room_m', 'room_dining_space_m', 'room_toilet_m', 'room_garden_room_m', 'room_cellar_one_m', 'room_cellar_two_m', 'room_cellar_three_m', 'room_cellar_four_m']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd74b2e4",
        "outputId": "2e6dbca9-75de-4ef5-a824-56b97536b04b"
      },
      "source": [
        "# @title Step 2: Select Relevant Columns\n",
        "# @markdown Since zoopla scraper scraps many irrelevent columns, we need to select one which actually beneficial for our rag pipeline these includes following columns , most import ones are adresses, property type, lat lang, nearest schools and stations\n",
        "\n",
        "print(\"üîÑ Step 2: Selecting relevant columns...\")\n",
        "\n",
        "# Define columns to keep (in order of importance)\n",
        "columns_to_keep = [\n",
        "    \"property_id\",\n",
        "    \"property_url\",\n",
        "    \"price\",\n",
        "    \"property_type\",\n",
        "    \"tenure\",\n",
        "    \"bedrooms\",\n",
        "    \"bathrooms\",\n",
        "    \"receptions\",\n",
        "    \"outcode\",\n",
        "    \"chain_free\",\n",
        "    \"number_of_photos\",\n",
        "    \"number_of_floorplans\",\n",
        "    \"address\",\n",
        "    \"council_tax_band\",\n",
        "    \"ground_rent\",\n",
        "    \"nearest_stations\",\n",
        "    \"nearest_schools\",\n",
        "    \"latitude\",\n",
        "    \"longitude\",\n",
        "    \"epc_rating\",\n",
        "    \"size_sqft\",\n",
        "    \"title\",\n",
        "    \"agent\",\n",
        "    \"about_property\",\n",
        "    # Crime data fields\n",
        "    \"crime_summary\",\n",
        "    \"crime_data\",\n",
        "]\n",
        "\n",
        "# Filter to only keep columns that exist in the DataFrame\n",
        "existing_columns = [col for col in columns_to_keep if col in df.columns]\n",
        "df_cleaned = df[existing_columns].copy()\n",
        "\n",
        "print(f\"‚úì Selected {len(existing_columns)} relevant columns\")\n",
        "print(f\"   Kept columns: {existing_columns}\")"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Step 2: Selecting relevant columns...\n",
            "‚úì Selected 23 relevant columns\n",
            "   Kept columns: ['property_id', 'property_url', 'price', 'property_type', 'tenure', 'bedrooms', 'bathrooms', 'receptions', 'outcode', 'chain_free', 'number_of_photos', 'number_of_floorplans', 'address', 'council_tax_band', 'ground_rent', 'nearest_stations', 'nearest_schools', 'latitude', 'longitude', 'epc_rating', 'title', 'agent', 'about_property']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19882647",
        "outputId": "e66947a2-a798-4ff6-a5ab-e967f37d2557"
      },
      "source": [
        "# @title Step 3: Rename Fields\n",
        "# @markdown We applied better naming conventions to columns\n",
        "\n",
        "print(\"üîÑ Step 3: Renaming fields...\")\n",
        "\n",
        "# Field mapping for better naming\n",
        "field_mapping = {\n",
        "    \"outcode\": \"postcode\",\n",
        "    \"latitude\": \"lat\",\n",
        "    \"longitude\": \"lng\",\n",
        "    \"agent\": \"agent_name\",\n",
        "    \"about_property\": \"description\",\n",
        "}\n",
        "\n",
        "# Rename fields that exist\n",
        "renamed_fields = []\n",
        "for old_name, new_name in field_mapping.items():\n",
        "    if old_name in df_cleaned.columns:\n",
        "        df_cleaned = df_cleaned.rename(columns={old_name: new_name})\n",
        "        renamed_fields.append(f\"{old_name} ‚Üí {new_name}\")\n",
        "\n",
        "if renamed_fields:\n",
        "    print(\"‚úì Fields renamed:\")\n",
        "    for rename in renamed_fields:\n",
        "        print(f\"   {rename}\")\n",
        "else:\n",
        "    print(\"‚úì No fields needed renaming\")"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Step 3: Renaming fields...\n",
            "‚úì Fields renamed:\n",
            "   outcode ‚Üí postcode\n",
            "   latitude ‚Üí lat\n",
            "   longitude ‚Üí lng\n",
            "   agent ‚Üí agent_name\n",
            "   about_property ‚Üí description\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0247c32",
        "outputId": "e1090906-02b3-47a4-a28d-248122531358"
      },
      "source": [
        "# @title Step 4: Handle Missing Values\n",
        "# @markdown Missing data would over croud our RAG pipeline and we need to Identify and handle missing data\n",
        "\n",
        "print(\"üîÑ Step 4: Handling missing values...\")\n",
        "\n",
        "# Check for missing values initially\n",
        "missing_counts = df_cleaned.isnull().sum()\n",
        "missing_data = missing_counts[missing_counts > 0]\n",
        "\n",
        "if not missing_data.empty:\n",
        "    print(\"‚ö†Ô∏è  Missing values found:\")\n",
        "    for col, count in missing_data.items():\n",
        "        percentage = (count / len(df_cleaned)) * 100\n",
        "        print(f\"   {col}: {count} ({percentage:.1f}%)\")\n",
        "else:\n",
        "    print(\"‚úì No missing values found\")\n",
        "\n",
        "# Impute missing values with mode for categorical columns\n",
        "categorical_cols = [\n",
        "    \"bathrooms\",\n",
        "    \"receptions\",\n",
        "    \"property_type\",\n",
        "    \"council_tax_band\",\n",
        "    \"tenure\",\n",
        "]\n",
        "\n",
        "filled_columns = []\n",
        "for col in categorical_cols:\n",
        "    if col in df_cleaned.columns and df_cleaned[col].isnull().sum() > 0:\n",
        "        mode_val = df_cleaned[col].mode()\n",
        "        if not mode_val.empty:\n",
        "            df_cleaned[col] = df_cleaned[col].fillna(mode_val[0])\n",
        "            filled_columns.append(f\"{col} (mode: {mode_val[0]})\")\n",
        "\n",
        "if filled_columns:\n",
        "    print(\"‚úì Filled missing values:\")\n",
        "    for fill in filled_columns:\n",
        "        print(f\"   {fill}\")\n",
        "else:\n",
        "    print(\"‚úì No categorical columns needed filling\")"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Step 4: Handling missing values...\n",
            "‚ö†Ô∏è  Missing values found:\n",
            "   price: 4 (0.4%)\n",
            "   property_type: 21 (2.3%)\n",
            "   tenure: 48 (5.3%)\n",
            "   bedrooms: 8 (0.9%)\n",
            "   bathrooms: 11 (1.2%)\n",
            "   receptions: 24 (2.6%)\n",
            "   postcode: 4 (0.4%)\n",
            "   chain_free: 4 (0.4%)\n",
            "   number_of_photos: 4 (0.4%)\n",
            "   number_of_floorplans: 28 (3.1%)\n",
            "   address: 4 (0.4%)\n",
            "   council_tax_band: 172 (18.9%)\n",
            "   ground_rent: 564 (62.1%)\n",
            "   nearest_stations: 4 (0.4%)\n",
            "   nearest_schools: 79 (8.7%)\n",
            "   lat: 4 (0.4%)\n",
            "   lng: 4 (0.4%)\n",
            "   epc_rating: 239 (26.3%)\n",
            "   title: 4 (0.4%)\n",
            "   agent_name: 4 (0.4%)\n",
            "   description: 4 (0.4%)\n",
            "‚úì Filled missing values:\n",
            "   bathrooms (mode: 1.0)\n",
            "   receptions (mode: 1.0)\n",
            "   property_type (mode: semi_detached)\n",
            "   council_tax_band (mode: A)\n",
            "   tenure (mode: freehold)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4db6b045",
        "outputId": "efafa036-66d5-4394-f163-eed7c85dd1ce"
      },
      "source": [
        "# @title Step 5: Convert Data Types\n",
        "# @markdown Since we have planned to apply self retrievel in our RAG and that work best with the attributes that are properly data types so we need to convert columns to appropriate data types\n",
        "\n",
        "print(\"üîÑ Step 5: Converting data types...\")\n",
        "\n",
        "# Convert numerical columns (exclude price; we'll build price_int later)\n",
        "numeric_cols = [\"bathrooms\", \"bedrooms\", \"receptions\", \"size_sqft\"]\n",
        "converted_cols = []\n",
        "\n",
        "for col in numeric_cols:\n",
        "    if col in df_cleaned.columns:\n",
        "        original_type = df_cleaned[col].dtype\n",
        "        df_cleaned[col] = pd.to_numeric(df_cleaned[col], errors=\"coerce\")\n",
        "        converted_cols.append(f\"{col}: {original_type} ‚Üí {df_cleaned[col].dtype}\")\n",
        "\n",
        "if converted_cols:\n",
        "    print(\"‚úì Data types converted:\")\n",
        "    for conversion in converted_cols:\n",
        "        print(f\"   {conversion}\")\n",
        "else:\n",
        "    print(\"‚úì No numeric columns needed conversion\")"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Step 5: Converting data types...\n",
            "‚úì Data types converted:\n",
            "   bathrooms: float64 ‚Üí float64\n",
            "   bedrooms: float64 ‚Üí float64\n",
            "   receptions: float64 ‚Üí float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92552972",
        "outputId": "d47829de-0143-4f47-9c8e-cb06dc4cb3cd"
      },
      "source": [
        "# @title Step 6: Clean Text Data\n",
        "# @markdown We'll use utility functions we craeted earlier to clean our data set which includes following columns which we know have white spaces etc\n",
        "print(\"üîÑ Step 6: Cleaning text data...\")\n",
        "\n",
        "\n",
        "def clean_text(text) -> str:\n",
        "    \"\"\"Clean text data by removing extra whitespace and newlines.\"\"\"\n",
        "    if isinstance(text, str):\n",
        "        # Remove extra whitespace and newline characters\n",
        "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "        return text\n",
        "    return text\n",
        "\n",
        "# Clean text data (exclude URLs and coordinates)\n",
        "text_cols = [\n",
        "    \"description\",\n",
        "    \"address\",\n",
        "    \"title\",\n",
        "    \"agent_name\",\n",
        "    \"nearest_stations\",\n",
        "    \"nearest_schools\",\n",
        "    \"epc_rating\",\n",
        "    \"council_tax_band\",\n",
        "]\n",
        "\n",
        "cleaned_text_cols = []\n",
        "for col in text_cols:\n",
        "    if col in df_cleaned.columns:\n",
        "        # Show example before cleaning\n",
        "        if not df_cleaned[col].dropna().empty:\n",
        "            sample_before = df_cleaned[col].dropna().iloc[0][:50] + \"...\" if len(str(df_cleaned[col].dropna().iloc[0])) > 50 else str(df_cleaned[col].dropna().iloc[0])\n",
        "\n",
        "        df_cleaned[col] = df_cleaned[col].apply(clean_text)\n",
        "        cleaned_text_cols.append(col)\n",
        "\n",
        "if cleaned_text_cols:\n",
        "    print(f\"‚úì Cleaned {len(cleaned_text_cols)} text columns:\")\n",
        "    for col in cleaned_text_cols:\n",
        "        print(f\"   {col}\")\n",
        "else:\n",
        "    print(\"‚úì No text columns needed cleaning\")"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Step 6: Cleaning text data...\n",
            "‚úì Cleaned 8 text columns:\n",
            "   description\n",
            "   address\n",
            "   title\n",
            "   agent_name\n",
            "   nearest_stations\n",
            "   nearest_schools\n",
            "   epc_rating\n",
            "   council_tax_band\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6258ff0c",
        "outputId": "61720b1d-22ab-4161-e311-54a862f68586"
      },
      "source": [
        "# @title Step 7: Create Helper Columns\n",
        "# @markdown Create numeric helper columns for better processing becaue we know we'll self query rag and it works better with typed data\n",
        "\n",
        "print(\"üîÑ Step 7: Creating numeric helper columns...\")\n",
        "\n",
        "helper_cols_created = []\n",
        "\n",
        "# Create numeric helper columns\n",
        "if \"bedrooms\" in df_cleaned.columns:\n",
        "    df_cleaned[\"bedrooms_int\"] = df_cleaned[\"bedrooms\"].astype(\"Int64\")\n",
        "    helper_cols_created.append(\"bedrooms_int\")\n",
        "\n",
        "if \"bathrooms\" in df_cleaned.columns:\n",
        "    df_cleaned[\"bathrooms_int\"] = df_cleaned[\"bathrooms\"].astype(\"Int64\")\n",
        "    helper_cols_created.append(\"bathrooms_int\")\n",
        "\n",
        "if \"receptions\" in df_cleaned.columns:\n",
        "    df_cleaned[\"receptions_int\"] = df_cleaned[\"receptions\"].astype(\"Int64\")\n",
        "    helper_cols_created.append(\"receptions_int\")\n",
        "\n",
        "if \"price\" in df_cleaned.columns:\n",
        "    # Build integer-only price for downstream use and vector DB metadata filtering\n",
        "    df_cleaned[\"price_int\"] = (\n",
        "        pd.to_numeric(df_cleaned[\"price\"], errors=\"coerce\").round(0).astype(\"Int64\")\n",
        "    )\n",
        "    helper_cols_created.append(\"price_int\")\n",
        "\n",
        "if \"size_sqft\" in df_cleaned.columns:\n",
        "    df_cleaned[\"size_sqft_num\"] = df_cleaned[\"size_sqft\"].astype(\"float64\")\n",
        "    helper_cols_created.append(\"size_sqft_num\")\n",
        "\n",
        "print(f\"‚úì Created {len(helper_cols_created)} helper columns:\")\n",
        "for col in helper_cols_created:\n",
        "    print(f\"   {col}\")\n",
        "\n",
        "# Keep only integer price; drop original and float helper if present\n",
        "cols_to_drop = [c for c in [\"price\", \"price_num\"] if c in df_cleaned.columns]\n",
        "if cols_to_drop:\n",
        "    df_cleaned = df_cleaned.drop(columns=cols_to_drop)\n",
        "    print(f\"‚úì Dropped original columns: {cols_to_drop}\")"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Step 7: Creating numeric helper columns...\n",
            "‚úì Created 4 helper columns:\n",
            "   bedrooms_int\n",
            "   bathrooms_int\n",
            "   receptions_int\n",
            "   price_int\n",
            "‚úì Dropped original columns: ['price']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53c774a6",
        "outputId": "995d25c6-c7a9-484e-b1cc-8ee87780b1a3"
      },
      "source": [
        "# @title Step 8: Filter Incomplete Properties\n",
        "\n",
        "print(\"üîÑ Step 8: Filtering out incomplete properties...\")\n",
        "\n",
        "# Filter out properties where essential fields are null\n",
        "initial_count = len(df_cleaned)\n",
        "\n",
        "# Remove properties with missing address, price_int, or description\n",
        "df_cleaned = df_cleaned.dropna(subset=[\"address\", \"price_int\", \"description\"])\n",
        "\n",
        "filtered_count = len(df_cleaned)\n",
        "removed_count = initial_count - filtered_count\n",
        "\n",
        "if removed_count > 0:\n",
        "    print(f\"‚ö†Ô∏è  Removed {removed_count} incomplete properties\")\n",
        "    print(f\"   (missing address, price_int, or description)\")\n",
        "    print(f\"‚úì Kept {filtered_count} complete properties\")\n",
        "else:\n",
        "    print(\"‚úì All properties have complete essential data\")"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Step 8: Filtering out incomplete properties...\n",
            "‚ö†Ô∏è  Removed 4 incomplete properties\n",
            "   (missing address, price_int, or description)\n",
            "‚úì Kept 904 complete properties\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3e464013",
        "outputId": "b3773f8b-3eb4-47d7-e1a0-00335286e5c7"
      },
      "source": [
        "# @title Step 9: Data Quality Summary\n",
        "# @markdown Review the final cleaned dataset\n",
        "\n",
        "print(\"üìä Step 9: Data Quality Summary\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "print(f\"üìà Dataset Overview:\")\n",
        "print(f\"   Final shape: {df_cleaned.shape}\")\n",
        "print(f\"   Properties: {len(df_cleaned)}\")\n",
        "print(f\"   Features: {len(df_cleaned.columns)}\")\n",
        "\n",
        "print(f\"\\nüí∞ Price Statistics:\")\n",
        "if \"price_int\" in df_cleaned.columns:\n",
        "    price_stats = df_cleaned[\"price_int\"].describe()\n",
        "    print(f\"   Min: ¬£{price_stats['min']:,.0f}\")\n",
        "    print(f\"   Max: ¬£{price_stats['max']:,.0f}\")\n",
        "    print(f\"   Mean: ¬£{price_stats['mean']:,.0f}\")\n",
        "    print(f\"   Median: ¬£{price_stats['50%']:,.0f}\")\n",
        "\n",
        "print(f\"\\nüè† Property Types:\")\n",
        "if \"property_type\" in df_cleaned.columns:\n",
        "    prop_counts = df_cleaned[\"property_type\"].value_counts()\n",
        "    for prop_type, count in prop_counts.head().items():\n",
        "        percentage = (count / len(df_cleaned)) * 100\n",
        "        print(f\"   {prop_type}: {count} ({percentage:.1f}%)\")\n",
        "\n",
        "print(f\"\\nüõèÔ∏è  Bedroom Distribution:\")\n",
        "if \"bedrooms_int\" in df_cleaned.columns:\n",
        "    bedroom_counts = df_cleaned[\"bedrooms_int\"].value_counts().sort_index()\n",
        "    for bedrooms, count in bedroom_counts.head().items():\n",
        "        percentage = (count / len(df_cleaned)) * 100\n",
        "        print(f\"   {bedrooms} bed: {count} ({percentage:.1f}%)\")\n",
        "\n",
        "print(f\"\\nüìç Location Coverage:\")\n",
        "if \"postcode\" in df_cleaned.columns:\n",
        "    postcode_count = df_cleaned[\"postcode\"].nunique()\n",
        "    print(f\"   Unique postcodes: {postcode_count}\")\n",
        "\n",
        "# Check final data quality\n",
        "final_missing = df_cleaned.isnull().sum()\n",
        "final_missing_data = final_missing[final_missing > 0]\n",
        "\n",
        "if not final_missing_data.empty:\n",
        "    print(f\"\\n‚ö†Ô∏è  Remaining missing values:\")\n",
        "    for col, count in final_missing_data.items():\n",
        "        percentage = (count / len(df_cleaned)) * 100\n",
        "        print(f\"   {col}: {count} ({percentage:.1f}%)\")\n",
        "else:\n",
        "    print(f\"\\n‚úì No missing values in essential columns\")"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Step 9: Data Quality Summary\n",
            "==================================================\n",
            "üìà Dataset Overview:\n",
            "   Final shape: (904, 26)\n",
            "   Properties: 904\n",
            "   Features: 26\n",
            "\n",
            "üí∞ Price Statistics:\n",
            "   Min: ¬£35,000\n",
            "   Max: ¬£2,000,000\n",
            "   Mean: ¬£328,743\n",
            "   Median: ¬£270,000\n",
            "\n",
            "üè† Property Types:\n",
            "   semi_detached: 286 (31.6%)\n",
            "   detached: 186 (20.6%)\n",
            "   flat: 155 (17.1%)\n",
            "   terraced: 148 (16.4%)\n",
            "   end_terrace: 27 (3.0%)\n",
            "\n",
            "üõèÔ∏è  Bedroom Distribution:\n",
            "   1 bed: 57 (6.3%)\n",
            "   2 bed: 257 (28.4%)\n",
            "   3 bed: 340 (37.6%)\n",
            "   4 bed: 175 (19.4%)\n",
            "   5 bed: 52 (5.8%)\n",
            "\n",
            "üìç Location Coverage:\n",
            "   Unique postcodes: 87\n",
            "\n",
            "‚ö†Ô∏è  Remaining missing values:\n",
            "   bedrooms: 4 (0.4%)\n",
            "   number_of_floorplans: 24 (2.7%)\n",
            "   ground_rent: 560 (61.9%)\n",
            "   nearest_schools: 75 (8.3%)\n",
            "   epc_rating: 235 (26.0%)\n",
            "   bedrooms_int: 4 (0.4%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7a9f45bb",
        "outputId": "5d0e1101-5e87-437a-cb6b-0ff43176af2d"
      },
      "source": [
        "# @title Step 10: Save Cleaned Data\n",
        "# @markdown Export the cleaned dataset to CSV and JSON formats\n",
        "\n",
        "print(\"üîÑ Step 10: Saving cleaned data...\")\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Generate output filenames\n",
        "base_name = os.path.splitext(os.path.basename(JSON_FILE_PATH))[0]\n",
        "csv_path = os.path.join(OUTPUT_DIR, f\"{base_name}_cleaned.csv\")\n",
        "json_path = os.path.join(OUTPUT_DIR, f\"{base_name}_cleaned.json\")\n",
        "\n",
        "# Save files\n",
        "try:\n",
        "    df_cleaned.to_csv(csv_path, index=False)\n",
        "    df_cleaned.to_json(json_path, indent=2, orient=\"records\")\n",
        "\n",
        "    print(\"‚úÖ Cleaned data saved successfully!\")\n",
        "    print(f\"   üìÑ CSV: {csv_path}\")\n",
        "    print(f\"   üìÑ JSON: {json_path}\")\n",
        "\n",
        "    # Show file sizes\n",
        "    csv_size = os.path.getsize(csv_path) / 1024 / 1024  # MB\n",
        "    json_size = os.path.getsize(json_path) / 1024 / 1024  # MB\n",
        "\n",
        "    print(f\"   üìä CSV size: {csv_size:.1f} MB\")\n",
        "    print(f\"   üìä JSON size: {json_size:.1f} MB\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error saving files: {e}\")"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Step 10: Saving cleaned data...\n",
            "‚úÖ Cleaned data saved successfully!\n",
            "   üìÑ CSV: ready_data/19-august-12am_page1-ALL_property1-908_cleaned.csv\n",
            "   üìÑ JSON: ready_data/19-august-12am_page1-ALL_property1-908_cleaned.json\n",
            "   üìä CSV size: 3.7 MB\n",
            "   üìä JSON size: 4.2 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6e077ef",
        "outputId": "94a8e193-b4de-4e1d-91b0-ed8a9a46f2ce"
      },
      "source": [
        "# @title Final Results\n",
        "# @markdown Display final processing results\n",
        "\n",
        "print(\"DATA PREPROCESSING COMPLETED!\")\n",
        "\n",
        "print(f\"\\n Processing Summary:\")\n",
        "print(f\"   Original records: {df.shape[0] if 'df' in locals() else 'N/A'}\")\n",
        "print(f\"   Final records: {len(df_cleaned)}\")\n",
        "print(f\"   Features: {len(df_cleaned.columns)}\")\n",
        "print(f\"   Data quality: {'High' if final_missing_data.empty else 'Good'}\")\n",
        "\n",
        "print(f\"\\n Output Files:\")\n",
        "print(f\"   üìÑ {csv_path}\")\n",
        "print(f\"   üìÑ {json_path}\")\n",
        "\n",
        "print(f\"\\n Next Steps:\")\n",
        "print(\"   1. Download the cleaned files from the output directory\")\n",
        "print(\"   2. Use the cleaned data for vector embedding\")\n",
        "print(\"   3. Set up your RAG pipeline with the processed data\")\n",
        "print(\"   4. Consider additional feature engineering if needed\")\n",
        "\n",
        "# Display sample of cleaned data\n",
        "print(f\"\\nüëÄ Sample of Cleaned Data:\")\n",
        "print(df_cleaned.head(3))"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DATA PREPROCESSING COMPLETED!\n",
            "\n",
            " Processing Summary:\n",
            "   Original records: 908\n",
            "   Final records: 904\n",
            "   Features: 26\n",
            "   Data quality: Good\n",
            "\n",
            " Output Files:\n",
            "   üìÑ ready_data/19-august-12am_page1-ALL_property1-908_cleaned.csv\n",
            "   üìÑ ready_data/19-august-12am_page1-ALL_property1-908_cleaned.json\n",
            "\n",
            " Next Steps:\n",
            "   1. Download the cleaned files from the output directory\n",
            "   2. Use the cleaned data for vector embedding\n",
            "   3. Set up your RAG pipeline with the processed data\n",
            "   4. Consider additional feature engineering if needed\n",
            "\n",
            "üëÄ Sample of Cleaned Data:\n",
            "   property_id                                       property_url  \\\n",
            "0     71084004  https://www.zoopla.co.uk/for-sale/details/7108...   \n",
            "1     71082046  https://www.zoopla.co.uk/for-sale/details/7108...   \n",
            "2     69999191  https://www.zoopla.co.uk/for-sale/details/6999...   \n",
            "\n",
            "  property_type     tenure  bedrooms  bathrooms  receptions postcode  \\\n",
            "0      terraced  leasehold       2.0        1.0         1.0      BL1   \n",
            "1          flat  leasehold       1.0        1.0         1.0       M3   \n",
            "2      detached   freehold       5.0        3.0         1.0      SK4   \n",
            "\n",
            "  chain_free  number_of_photos  ...        lat       lng epc_rating  \\\n",
            "0       True              13.0  ...  53.577600 -2.457513        NaN   \n",
            "1      False              12.0  ...  53.483557 -2.254223        NaN   \n",
            "2      False              20.0  ...  53.414359 -2.193309          B   \n",
            "\n",
            "                                               title  \\\n",
            "0  2 bed terraced house for sale Third Avenue, Bo...   \n",
            "1  1 bed flat for sale Albert Vaults, 169-171, Ch...   \n",
            "2  5 bed detached house for sale 42 Queens Drive SK4   \n",
            "\n",
            "                             agent_name  \\\n",
            "0  Lancasters Independent Estate Agents   \n",
            "1      Springbok Properties, Nationwide   \n",
            "2                     Metro Residential   \n",
            "\n",
            "                                         description  bedrooms_int  \\\n",
            "0  Stunning terraced home available with no chain...             2   \n",
            "1  Council Tax Band A, Leasehold 250 years from a...             1   \n",
            "2  Kitchen-Diner Garden En-suite Full Double Glaz...             5   \n",
            "\n",
            "   bathrooms_int receptions_int price_int  \n",
            "0              1              1    165000  \n",
            "1              1              1    110000  \n",
            "2              3              1    885000  \n",
            "\n",
            "[3 rows x 26 columns]\n"
          ]
        }
      ]
    }
  ]
}