{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ijBsPrWwl_Wq"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Real Estate Data Preprocessing Pipeline\n",
        "Notebook for cleaning and preparing property data for RAG pipeline\n"
      ],
      "metadata": {
        "id": "ijBsPrWwl_Wq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "from typing import Optional, List\n",
        "\n",
        "print(\"âœ“ Libraries imported successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-ZS_CAflUUb",
        "outputId": "0b4f1719-e3c3-4721-ecc7-b6f342b3e564"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Libraries imported successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71f4360b"
      },
      "source": [],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34f1d3c9",
        "outputId": "22e56f38-047a-40bc-9ef4-b109f885348b"
      },
      "source": [
        "JSON_FILE_PATH = \"/content/19-august-12am_page1-ALL_property1-908.json\"\n",
        "OUTPUT_DIR = \"ready_data\"\n",
        "\n",
        "try:\n",
        "    df = pd.read_json(JSON_FILE_PATH)\n",
        "    print(\"âœ“ JSON file loaded successfully!\")\n",
        "    print(f\"   Original dataset shape: {df.shape}\")\n",
        "    print(f\"   Columns found: {list(df.columns)}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"âŒ File not found: {JSON_FILE_PATH}\")\n",
        "    print(\"Please upload your JSON file to Colab and update the path above\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error loading file: {e}\")"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ JSON file loaded successfully!\n",
            "   Original dataset shape: (908, 134)\n",
            "   Columns found: ['page', 'scraped_at', 'source', 'property_id', 'property_url', 'listing_id', 'price', 'property_type', 'tenure', 'bedrooms', 'bathrooms', 'receptions', 'has_epc', 'has_floorplan', 'display_address', 'outcode', 'agent', 'chain_free', 'title', 'status', 'number_of_photos', 'number_of_floorplans', 'address', 'council_tax_band', 'ground_rent', 'room__m', 'about_property', 'nearest_schools', 'nearest_stations', 'latitude', 'longitude', 'nearest_stations_distances', 'epc_rating', 'size_sq_feet', 'price_per_sqft', 'room_lounge_m', 'room_dining_room_m', 'room_office_m', 'room_kitchen_m', 'room_bedroom_one_m', 'room_bedroom_two_m', 'room_bedroom_three_m', 'room_bedroom_four_m', 'room_family_bathroom_m', 'room_master_bedroom_m', 'room_shower_room_m', 'room_bathroom_m', 'room_en_suite_m', 'room_porch_m', 'room_entrance_hallway_m', 'room_diner_m', 'room_reception_room_one_m', 'room_reception_room_two_m', 'room_kitchen_diner_m', 'room_utility_room_m', 'room_hall_m', 'room_dining_m', 'room_landing_m', 'room_dining_kitchen_m', 'room_bedroom_m', 'room_suite_m', 'room_entrance_vestibule_m', 'room_living_room_m', 'room_entrance_porch_m', 'room_orangery_m', 'room_under_stairs_storage_m', 'room_first_floor_landing_m', 'room_primary_bedroom_m', 'room_secondary_bedroom_m', 'room_dining_area_m', 'room_wet_room_m', 'room_conservatory_m', 'room_entry_m', 'room_sunroom_m', 'room_hallway_m', 'room_study_m', 'room_wc_m', 'room_c_m', 'room_ensuite_m', 'room_cellar_m', 'room_dressing_room_m', 'room_entrance_hall_m', 'room_family_room_m', 'room_utility_cupboard_m', 'room_workshop_m', 'room_utility_m', 'room_sun_room_m', 'room_bedroom_five_m', 'room_loft_room_m', 'room_garage_m', 'room_understairs_storage_m', 'room_downstairs_shower_room_m', 'room_basement_m', 'room_max_to_alcove_m', 'room_max_into_shower_recess_itself_m', 'room_to_the_cove_m', 'room_sitting_area_m', 'room_second_bedroom_m', 'room_third_bedroom_m', 'room_downstairs_wc_m', 'room_morning_room_m', 'room_open_plan_lounge_and_kitchen_m', 'room_second_reception_room_m', 'room_office_room_m', 'room_double_garage_m', 'room_sitting_room_m', 'room_laundry_m', 'room_ground_floor_m', 'room_floor_one_m', 'room_storage_m', 'room_landing_two_m', 'room_master_bedroom_and_en_suite_m', 'room_entrance_m', 'room_reception_room_m', 'room_snug_m', 'room_lounge_diner_m', 'room_kitchen_and_dining_room_m', 'room_reception_one_m', 'room_reception_two_m', 'room_reception_three_m', 'room_guest_cloakroom_m', 'room_fourth_bedroom_m', 'room_stairway_m', 'room_integral_garage_m', 'room_second_floor_landing_m', 'room_reception_room_three_m', 'room_play_room_m', 'room_dining_space_m', 'room_toilet_m', 'room_garden_room_m', 'room_cellar_one_m', 'room_cellar_two_m', 'room_cellar_three_m', 'room_cellar_four_m']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd74b2e4",
        "outputId": "2e6dbca9-75de-4ef5-a824-56b97536b04b"
      },
      "source": [
        "# @title Step 2: Select Relevant Columns\n",
        "# @markdown Since zoopla scraper scraps many irrelevent columns, we need to select one which actually beneficial for our rag pipeline these includes following columns , most import ones are adresses, property type, lat lang, nearest schools and stations\n",
        "\n",
        "print(\"ğŸ”„ Step 2: Selecting relevant columns...\")\n",
        "\n",
        "# Define columns to keep (in order of importance)\n",
        "columns_to_keep = [\n",
        "    \"property_id\",\n",
        "    \"property_url\",\n",
        "    \"price\",\n",
        "    \"property_type\",\n",
        "    \"tenure\",\n",
        "    \"bedrooms\",\n",
        "    \"bathrooms\",\n",
        "    \"receptions\",\n",
        "    \"outcode\",\n",
        "    \"chain_free\",\n",
        "    \"number_of_photos\",\n",
        "    \"number_of_floorplans\",\n",
        "    \"address\",\n",
        "    \"council_tax_band\",\n",
        "    \"ground_rent\",\n",
        "    \"nearest_stations\",\n",
        "    \"nearest_schools\",\n",
        "    \"latitude\",\n",
        "    \"longitude\",\n",
        "    \"epc_rating\",\n",
        "    \"size_sqft\",\n",
        "    \"title\",\n",
        "    \"agent\",\n",
        "    \"about_property\",\n",
        "    # Crime data fields\n",
        "    \"crime_summary\",\n",
        "    \"crime_data\",\n",
        "]\n",
        "\n",
        "# Filter to only keep columns that exist in the DataFrame\n",
        "existing_columns = [col for col in columns_to_keep if col in df.columns]\n",
        "df_cleaned = df[existing_columns].copy()\n",
        "\n",
        "print(f\"âœ“ Selected {len(existing_columns)} relevant columns\")\n",
        "print(f\"   Kept columns: {existing_columns}\")"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”„ Step 2: Selecting relevant columns...\n",
            "âœ“ Selected 23 relevant columns\n",
            "   Kept columns: ['property_id', 'property_url', 'price', 'property_type', 'tenure', 'bedrooms', 'bathrooms', 'receptions', 'outcode', 'chain_free', 'number_of_photos', 'number_of_floorplans', 'address', 'council_tax_band', 'ground_rent', 'nearest_stations', 'nearest_schools', 'latitude', 'longitude', 'epc_rating', 'title', 'agent', 'about_property']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19882647",
        "outputId": "e66947a2-a798-4ff6-a5ab-e967f37d2557"
      },
      "source": [
        "# @title Step 3: Rename Fields\n",
        "# @markdown We applied better naming conventions to columns\n",
        "\n",
        "print(\"ğŸ”„ Step 3: Renaming fields...\")\n",
        "\n",
        "# Field mapping for better naming\n",
        "field_mapping = {\n",
        "    \"outcode\": \"postcode\",\n",
        "    \"latitude\": \"lat\",\n",
        "    \"longitude\": \"lng\",\n",
        "    \"agent\": \"agent_name\",\n",
        "    \"about_property\": \"description\",\n",
        "}\n",
        "\n",
        "# Rename fields that exist\n",
        "renamed_fields = []\n",
        "for old_name, new_name in field_mapping.items():\n",
        "    if old_name in df_cleaned.columns:\n",
        "        df_cleaned = df_cleaned.rename(columns={old_name: new_name})\n",
        "        renamed_fields.append(f\"{old_name} â†’ {new_name}\")\n",
        "\n",
        "if renamed_fields:\n",
        "    print(\"âœ“ Fields renamed:\")\n",
        "    for rename in renamed_fields:\n",
        "        print(f\"   {rename}\")\n",
        "else:\n",
        "    print(\"âœ“ No fields needed renaming\")"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”„ Step 3: Renaming fields...\n",
            "âœ“ Fields renamed:\n",
            "   outcode â†’ postcode\n",
            "   latitude â†’ lat\n",
            "   longitude â†’ lng\n",
            "   agent â†’ agent_name\n",
            "   about_property â†’ description\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0247c32",
        "outputId": "e1090906-02b3-47a4-a28d-248122531358"
      },
      "source": [
        "# @title Step 4: Handle Missing Values\n",
        "# @markdown Missing data would over croud our RAG pipeline and we need to Identify and handle missing data\n",
        "\n",
        "print(\"ğŸ”„ Step 4: Handling missing values...\")\n",
        "\n",
        "# Check for missing values initially\n",
        "missing_counts = df_cleaned.isnull().sum()\n",
        "missing_data = missing_counts[missing_counts > 0]\n",
        "\n",
        "if not missing_data.empty:\n",
        "    print(\"âš ï¸  Missing values found:\")\n",
        "    for col, count in missing_data.items():\n",
        "        percentage = (count / len(df_cleaned)) * 100\n",
        "        print(f\"   {col}: {count} ({percentage:.1f}%)\")\n",
        "else:\n",
        "    print(\"âœ“ No missing values found\")\n",
        "\n",
        "# Impute missing values with mode for categorical columns\n",
        "categorical_cols = [\n",
        "    \"bathrooms\",\n",
        "    \"receptions\",\n",
        "    \"property_type\",\n",
        "    \"council_tax_band\",\n",
        "    \"tenure\",\n",
        "]\n",
        "\n",
        "filled_columns = []\n",
        "for col in categorical_cols:\n",
        "    if col in df_cleaned.columns and df_cleaned[col].isnull().sum() > 0:\n",
        "        mode_val = df_cleaned[col].mode()\n",
        "        if not mode_val.empty:\n",
        "            df_cleaned[col] = df_cleaned[col].fillna(mode_val[0])\n",
        "            filled_columns.append(f\"{col} (mode: {mode_val[0]})\")\n",
        "\n",
        "if filled_columns:\n",
        "    print(\"âœ“ Filled missing values:\")\n",
        "    for fill in filled_columns:\n",
        "        print(f\"   {fill}\")\n",
        "else:\n",
        "    print(\"âœ“ No categorical columns needed filling\")"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”„ Step 4: Handling missing values...\n",
            "âš ï¸  Missing values found:\n",
            "   price: 4 (0.4%)\n",
            "   property_type: 21 (2.3%)\n",
            "   tenure: 48 (5.3%)\n",
            "   bedrooms: 8 (0.9%)\n",
            "   bathrooms: 11 (1.2%)\n",
            "   receptions: 24 (2.6%)\n",
            "   postcode: 4 (0.4%)\n",
            "   chain_free: 4 (0.4%)\n",
            "   number_of_photos: 4 (0.4%)\n",
            "   number_of_floorplans: 28 (3.1%)\n",
            "   address: 4 (0.4%)\n",
            "   council_tax_band: 172 (18.9%)\n",
            "   ground_rent: 564 (62.1%)\n",
            "   nearest_stations: 4 (0.4%)\n",
            "   nearest_schools: 79 (8.7%)\n",
            "   lat: 4 (0.4%)\n",
            "   lng: 4 (0.4%)\n",
            "   epc_rating: 239 (26.3%)\n",
            "   title: 4 (0.4%)\n",
            "   agent_name: 4 (0.4%)\n",
            "   description: 4 (0.4%)\n",
            "âœ“ Filled missing values:\n",
            "   bathrooms (mode: 1.0)\n",
            "   receptions (mode: 1.0)\n",
            "   property_type (mode: semi_detached)\n",
            "   council_tax_band (mode: A)\n",
            "   tenure (mode: freehold)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4db6b045",
        "outputId": "efafa036-66d5-4394-f163-eed7c85dd1ce"
      },
      "source": [
        "# @title Step 5: Convert Data Types\n",
        "# @markdown Since we have planned to apply self retrievel in our RAG and that work best with the attributes that are properly data types so we need to convert columns to appropriate data types\n",
        "\n",
        "print(\"ğŸ”„ Step 5: Converting data types...\")\n",
        "\n",
        "# Convert numerical columns (exclude price; we'll build price_int later)\n",
        "numeric_cols = [\"bathrooms\", \"bedrooms\", \"receptions\", \"size_sqft\"]\n",
        "converted_cols = []\n",
        "\n",
        "for col in numeric_cols:\n",
        "    if col in df_cleaned.columns:\n",
        "        original_type = df_cleaned[col].dtype\n",
        "        df_cleaned[col] = pd.to_numeric(df_cleaned[col], errors=\"coerce\")\n",
        "        converted_cols.append(f\"{col}: {original_type} â†’ {df_cleaned[col].dtype}\")\n",
        "\n",
        "if converted_cols:\n",
        "    print(\"âœ“ Data types converted:\")\n",
        "    for conversion in converted_cols:\n",
        "        print(f\"   {conversion}\")\n",
        "else:\n",
        "    print(\"âœ“ No numeric columns needed conversion\")"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”„ Step 5: Converting data types...\n",
            "âœ“ Data types converted:\n",
            "   bathrooms: float64 â†’ float64\n",
            "   bedrooms: float64 â†’ float64\n",
            "   receptions: float64 â†’ float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92552972",
        "outputId": "d47829de-0143-4f47-9c8e-cb06dc4cb3cd"
      },
      "source": [
        "# @title Step 6: Clean Text Data\n",
        "# @markdown We'll use utility functions we craeted earlier to clean our data set which includes following columns which we know have white spaces etc\n",
        "print(\"ğŸ”„ Step 6: Cleaning text data...\")\n",
        "\n",
        "\n",
        "def clean_text(text) -> str:\n",
        "    \"\"\"Clean text data by removing extra whitespace and newlines.\"\"\"\n",
        "    if isinstance(text, str):\n",
        "        # Remove extra whitespace and newline characters\n",
        "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "        return text\n",
        "    return text\n",
        "\n",
        "# Clean text data (exclude URLs and coordinates)\n",
        "text_cols = [\n",
        "    \"description\",\n",
        "    \"address\",\n",
        "    \"title\",\n",
        "    \"agent_name\",\n",
        "    \"nearest_stations\",\n",
        "    \"nearest_schools\",\n",
        "    \"epc_rating\",\n",
        "    \"council_tax_band\",\n",
        "]\n",
        "\n",
        "cleaned_text_cols = []\n",
        "for col in text_cols:\n",
        "    if col in df_cleaned.columns:\n",
        "        # Show example before cleaning\n",
        "        if not df_cleaned[col].dropna().empty:\n",
        "            sample_before = df_cleaned[col].dropna().iloc[0][:50] + \"...\" if len(str(df_cleaned[col].dropna().iloc[0])) > 50 else str(df_cleaned[col].dropna().iloc[0])\n",
        "\n",
        "        df_cleaned[col] = df_cleaned[col].apply(clean_text)\n",
        "        cleaned_text_cols.append(col)\n",
        "\n",
        "if cleaned_text_cols:\n",
        "    print(f\"âœ“ Cleaned {len(cleaned_text_cols)} text columns:\")\n",
        "    for col in cleaned_text_cols:\n",
        "        print(f\"   {col}\")\n",
        "else:\n",
        "    print(\"âœ“ No text columns needed cleaning\")"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”„ Step 6: Cleaning text data...\n",
            "âœ“ Cleaned 8 text columns:\n",
            "   description\n",
            "   address\n",
            "   title\n",
            "   agent_name\n",
            "   nearest_stations\n",
            "   nearest_schools\n",
            "   epc_rating\n",
            "   council_tax_band\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6258ff0c",
        "outputId": "61720b1d-22ab-4161-e311-54a862f68586"
      },
      "source": [
        "# @title Step 7: Create Helper Columns\n",
        "# @markdown Create numeric helper columns for better processing becaue we know we'll self query rag and it works better with typed data\n",
        "\n",
        "print(\"ğŸ”„ Step 7: Creating numeric helper columns...\")\n",
        "\n",
        "helper_cols_created = []\n",
        "\n",
        "# Create numeric helper columns\n",
        "if \"bedrooms\" in df_cleaned.columns:\n",
        "    df_cleaned[\"bedrooms_int\"] = df_cleaned[\"bedrooms\"].astype(\"Int64\")\n",
        "    helper_cols_created.append(\"bedrooms_int\")\n",
        "\n",
        "if \"bathrooms\" in df_cleaned.columns:\n",
        "    df_cleaned[\"bathrooms_int\"] = df_cleaned[\"bathrooms\"].astype(\"Int64\")\n",
        "    helper_cols_created.append(\"bathrooms_int\")\n",
        "\n",
        "if \"receptions\" in df_cleaned.columns:\n",
        "    df_cleaned[\"receptions_int\"] = df_cleaned[\"receptions\"].astype(\"Int64\")\n",
        "    helper_cols_created.append(\"receptions_int\")\n",
        "\n",
        "if \"price\" in df_cleaned.columns:\n",
        "    # Build integer-only price for downstream use and vector DB metadata filtering\n",
        "    df_cleaned[\"price_int\"] = (\n",
        "        pd.to_numeric(df_cleaned[\"price\"], errors=\"coerce\").round(0).astype(\"Int64\")\n",
        "    )\n",
        "    helper_cols_created.append(\"price_int\")\n",
        "\n",
        "if \"size_sqft\" in df_cleaned.columns:\n",
        "    df_cleaned[\"size_sqft_num\"] = df_cleaned[\"size_sqft\"].astype(\"float64\")\n",
        "    helper_cols_created.append(\"size_sqft_num\")\n",
        "\n",
        "print(f\"âœ“ Created {len(helper_cols_created)} helper columns:\")\n",
        "for col in helper_cols_created:\n",
        "    print(f\"   {col}\")\n",
        "\n",
        "# Keep only integer price; drop original and float helper if present\n",
        "cols_to_drop = [c for c in [\"price\", \"price_num\"] if c in df_cleaned.columns]\n",
        "if cols_to_drop:\n",
        "    df_cleaned = df_cleaned.drop(columns=cols_to_drop)\n",
        "    print(f\"âœ“ Dropped original columns: {cols_to_drop}\")"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”„ Step 7: Creating numeric helper columns...\n",
            "âœ“ Created 4 helper columns:\n",
            "   bedrooms_int\n",
            "   bathrooms_int\n",
            "   receptions_int\n",
            "   price_int\n",
            "âœ“ Dropped original columns: ['price']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53c774a6",
        "outputId": "995d25c6-c7a9-484e-b1cc-8ee87780b1a3"
      },
      "source": [
        "# @title Step 8: Filter Incomplete Properties\n",
        "\n",
        "print(\"ğŸ”„ Step 8: Filtering out incomplete properties...\")\n",
        "\n",
        "# Filter out properties where essential fields are null\n",
        "initial_count = len(df_cleaned)\n",
        "\n",
        "# Remove properties with missing address, price_int, or description\n",
        "df_cleaned = df_cleaned.dropna(subset=[\"address\", \"price_int\", \"description\"])\n",
        "\n",
        "filtered_count = len(df_cleaned)\n",
        "removed_count = initial_count - filtered_count\n",
        "\n",
        "if removed_count > 0:\n",
        "    print(f\"âš ï¸  Removed {removed_count} incomplete properties\")\n",
        "    print(f\"   (missing address, price_int, or description)\")\n",
        "    print(f\"âœ“ Kept {filtered_count} complete properties\")\n",
        "else:\n",
        "    print(\"âœ“ All properties have complete essential data\")"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”„ Step 8: Filtering out incomplete properties...\n",
            "âš ï¸  Removed 4 incomplete properties\n",
            "   (missing address, price_int, or description)\n",
            "âœ“ Kept 904 complete properties\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3e464013",
        "outputId": "b3773f8b-3eb4-47d7-e1a0-00335286e5c7"
      },
      "source": [
        "# @title Step 9: Data Quality Summary\n",
        "# @markdown Review the final cleaned dataset\n",
        "\n",
        "print(\"ğŸ“Š Step 9: Data Quality Summary\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "print(f\"ğŸ“ˆ Dataset Overview:\")\n",
        "print(f\"   Final shape: {df_cleaned.shape}\")\n",
        "print(f\"   Properties: {len(df_cleaned)}\")\n",
        "print(f\"   Features: {len(df_cleaned.columns)}\")\n",
        "\n",
        "print(f\"\\nğŸ’° Price Statistics:\")\n",
        "if \"price_int\" in df_cleaned.columns:\n",
        "    price_stats = df_cleaned[\"price_int\"].describe()\n",
        "    print(f\"   Min: Â£{price_stats['min']:,.0f}\")\n",
        "    print(f\"   Max: Â£{price_stats['max']:,.0f}\")\n",
        "    print(f\"   Mean: Â£{price_stats['mean']:,.0f}\")\n",
        "    print(f\"   Median: Â£{price_stats['50%']:,.0f}\")\n",
        "\n",
        "print(f\"\\nğŸ  Property Types:\")\n",
        "if \"property_type\" in df_cleaned.columns:\n",
        "    prop_counts = df_cleaned[\"property_type\"].value_counts()\n",
        "    for prop_type, count in prop_counts.head().items():\n",
        "        percentage = (count / len(df_cleaned)) * 100\n",
        "        print(f\"   {prop_type}: {count} ({percentage:.1f}%)\")\n",
        "\n",
        "print(f\"\\nğŸ›ï¸  Bedroom Distribution:\")\n",
        "if \"bedrooms_int\" in df_cleaned.columns:\n",
        "    bedroom_counts = df_cleaned[\"bedrooms_int\"].value_counts().sort_index()\n",
        "    for bedrooms, count in bedroom_counts.head().items():\n",
        "        percentage = (count / len(df_cleaned)) * 100\n",
        "        print(f\"   {bedrooms} bed: {count} ({percentage:.1f}%)\")\n",
        "\n",
        "print(f\"\\nğŸ“ Location Coverage:\")\n",
        "if \"postcode\" in df_cleaned.columns:\n",
        "    postcode_count = df_cleaned[\"postcode\"].nunique()\n",
        "    print(f\"   Unique postcodes: {postcode_count}\")\n",
        "\n",
        "# Check final data quality\n",
        "final_missing = df_cleaned.isnull().sum()\n",
        "final_missing_data = final_missing[final_missing > 0]\n",
        "\n",
        "if not final_missing_data.empty:\n",
        "    print(f\"\\nâš ï¸  Remaining missing values:\")\n",
        "    for col, count in final_missing_data.items():\n",
        "        percentage = (count / len(df_cleaned)) * 100\n",
        "        print(f\"   {col}: {count} ({percentage:.1f}%)\")\n",
        "else:\n",
        "    print(f\"\\nâœ“ No missing values in essential columns\")"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“Š Step 9: Data Quality Summary\n",
            "==================================================\n",
            "ğŸ“ˆ Dataset Overview:\n",
            "   Final shape: (904, 26)\n",
            "   Properties: 904\n",
            "   Features: 26\n",
            "\n",
            "ğŸ’° Price Statistics:\n",
            "   Min: Â£35,000\n",
            "   Max: Â£2,000,000\n",
            "   Mean: Â£328,743\n",
            "   Median: Â£270,000\n",
            "\n",
            "ğŸ  Property Types:\n",
            "   semi_detached: 286 (31.6%)\n",
            "   detached: 186 (20.6%)\n",
            "   flat: 155 (17.1%)\n",
            "   terraced: 148 (16.4%)\n",
            "   end_terrace: 27 (3.0%)\n",
            "\n",
            "ğŸ›ï¸  Bedroom Distribution:\n",
            "   1 bed: 57 (6.3%)\n",
            "   2 bed: 257 (28.4%)\n",
            "   3 bed: 340 (37.6%)\n",
            "   4 bed: 175 (19.4%)\n",
            "   5 bed: 52 (5.8%)\n",
            "\n",
            "ğŸ“ Location Coverage:\n",
            "   Unique postcodes: 87\n",
            "\n",
            "âš ï¸  Remaining missing values:\n",
            "   bedrooms: 4 (0.4%)\n",
            "   number_of_floorplans: 24 (2.7%)\n",
            "   ground_rent: 560 (61.9%)\n",
            "   nearest_schools: 75 (8.3%)\n",
            "   epc_rating: 235 (26.0%)\n",
            "   bedrooms_int: 4 (0.4%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7a9f45bb",
        "outputId": "5d0e1101-5e87-437a-cb6b-0ff43176af2d"
      },
      "source": [
        "# @title Step 10: Save Cleaned Data\n",
        "# @markdown Export the cleaned dataset to CSV and JSON formats\n",
        "\n",
        "print(\"ğŸ”„ Step 10: Saving cleaned data...\")\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Generate output filenames\n",
        "base_name = os.path.splitext(os.path.basename(JSON_FILE_PATH))[0]\n",
        "csv_path = os.path.join(OUTPUT_DIR, f\"{base_name}_cleaned.csv\")\n",
        "json_path = os.path.join(OUTPUT_DIR, f\"{base_name}_cleaned.json\")\n",
        "\n",
        "# Save files\n",
        "try:\n",
        "    df_cleaned.to_csv(csv_path, index=False)\n",
        "    df_cleaned.to_json(json_path, indent=2, orient=\"records\")\n",
        "\n",
        "    print(\"âœ… Cleaned data saved successfully!\")\n",
        "    print(f\"   ğŸ“„ CSV: {csv_path}\")\n",
        "    print(f\"   ğŸ“„ JSON: {json_path}\")\n",
        "\n",
        "    # Show file sizes\n",
        "    csv_size = os.path.getsize(csv_path) / 1024 / 1024  # MB\n",
        "    json_size = os.path.getsize(json_path) / 1024 / 1024  # MB\n",
        "\n",
        "    print(f\"   ğŸ“Š CSV size: {csv_size:.1f} MB\")\n",
        "    print(f\"   ğŸ“Š JSON size: {json_size:.1f} MB\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error saving files: {e}\")"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”„ Step 10: Saving cleaned data...\n",
            "âœ… Cleaned data saved successfully!\n",
            "   ğŸ“„ CSV: ready_data/19-august-12am_page1-ALL_property1-908_cleaned.csv\n",
            "   ğŸ“„ JSON: ready_data/19-august-12am_page1-ALL_property1-908_cleaned.json\n",
            "   ğŸ“Š CSV size: 3.7 MB\n",
            "   ğŸ“Š JSON size: 4.2 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6e077ef",
        "outputId": "94a8e193-b4de-4e1d-91b0-ed8a9a46f2ce"
      },
      "source": [
        "# @title Final Results\n",
        "# @markdown Display final processing results\n",
        "\n",
        "print(\"DATA PREPROCESSING COMPLETED!\")\n",
        "\n",
        "print(f\"\\n Processing Summary:\")\n",
        "print(f\"   Original records: {df.shape[0] if 'df' in locals() else 'N/A'}\")\n",
        "print(f\"   Final records: {len(df_cleaned)}\")\n",
        "print(f\"   Features: {len(df_cleaned.columns)}\")\n",
        "print(f\"   Data quality: {'High' if final_missing_data.empty else 'Good'}\")\n",
        "\n",
        "print(f\"\\n Output Files:\")\n",
        "print(f\"   ğŸ“„ {csv_path}\")\n",
        "print(f\"   ğŸ“„ {json_path}\")\n",
        "\n",
        "print(f\"\\n Next Steps:\")\n",
        "print(\"   1. Download the cleaned files from the output directory\")\n",
        "print(\"   2. Use the cleaned data for vector embedding\")\n",
        "print(\"   3. Set up your RAG pipeline with the processed data\")\n",
        "print(\"   4. Consider additional feature engineering if needed\")\n",
        "\n",
        "# Display sample of cleaned data\n",
        "print(f\"\\nğŸ‘€ Sample of Cleaned Data:\")\n",
        "print(df_cleaned.head(3))"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DATA PREPROCESSING COMPLETED!\n",
            "\n",
            " Processing Summary:\n",
            "   Original records: 908\n",
            "   Final records: 904\n",
            "   Features: 26\n",
            "   Data quality: Good\n",
            "\n",
            " Output Files:\n",
            "   ğŸ“„ ready_data/19-august-12am_page1-ALL_property1-908_cleaned.csv\n",
            "   ğŸ“„ ready_data/19-august-12am_page1-ALL_property1-908_cleaned.json\n",
            "\n",
            " Next Steps:\n",
            "   1. Download the cleaned files from the output directory\n",
            "   2. Use the cleaned data for vector embedding\n",
            "   3. Set up your RAG pipeline with the processed data\n",
            "   4. Consider additional feature engineering if needed\n",
            "\n",
            "ğŸ‘€ Sample of Cleaned Data:\n",
            "   property_id                                       property_url  \\\n",
            "0     71084004  https://www.zoopla.co.uk/for-sale/details/7108...   \n",
            "1     71082046  https://www.zoopla.co.uk/for-sale/details/7108...   \n",
            "2     69999191  https://www.zoopla.co.uk/for-sale/details/6999...   \n",
            "\n",
            "  property_type     tenure  bedrooms  bathrooms  receptions postcode  \\\n",
            "0      terraced  leasehold       2.0        1.0         1.0      BL1   \n",
            "1          flat  leasehold       1.0        1.0         1.0       M3   \n",
            "2      detached   freehold       5.0        3.0         1.0      SK4   \n",
            "\n",
            "  chain_free  number_of_photos  ...        lat       lng epc_rating  \\\n",
            "0       True              13.0  ...  53.577600 -2.457513        NaN   \n",
            "1      False              12.0  ...  53.483557 -2.254223        NaN   \n",
            "2      False              20.0  ...  53.414359 -2.193309          B   \n",
            "\n",
            "                                               title  \\\n",
            "0  2 bed terraced house for sale Third Avenue, Bo...   \n",
            "1  1 bed flat for sale Albert Vaults, 169-171, Ch...   \n",
            "2  5 bed detached house for sale 42 Queens Drive SK4   \n",
            "\n",
            "                             agent_name  \\\n",
            "0  Lancasters Independent Estate Agents   \n",
            "1      Springbok Properties, Nationwide   \n",
            "2                     Metro Residential   \n",
            "\n",
            "                                         description  bedrooms_int  \\\n",
            "0  Stunning terraced home available with no chain...             2   \n",
            "1  Council Tax Band A, Leasehold 250 years from a...             1   \n",
            "2  Kitchen-Diner Garden En-suite Full Double Glaz...             5   \n",
            "\n",
            "   bathrooms_int receptions_int price_int  \n",
            "0              1              1    165000  \n",
            "1              1              1    110000  \n",
            "2              3              1    885000  \n",
            "\n",
            "[3 rows x 26 columns]\n"
          ]
        }
      ]
    }
  ]
}